{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdsHKcQ-_zUM"
   },
   "source": [
    "# Extract and save features (Readability and NLTK features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13394,
     "status": "ok",
     "timestamp": 1604003728778,
     "user": {
      "displayName": "Anu Shrestha",
      "photoUrl": "",
      "userId": "15243898305987665146"
     },
     "user_tz": 360
    },
    "id": "9W_TfpW2_rsX",
    "outputId": "05a4b8af-0db8-4f6d-a1c6-ce1938443418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ReadabilityCalculator in /usr/local/lib/python3.6/dist-packages (0.2.37)\n",
      "Requirement already satisfied: justext>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from ReadabilityCalculator) (2.2.0)\n",
      "Requirement already satisfied: Pyphen>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from ReadabilityCalculator) (0.9.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.5.0 in /usr/local/lib/python3.6/dist-packages (from ReadabilityCalculator) (4.6.3)\n",
      "Requirement already satisfied: nltk>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from ReadabilityCalculator) (3.2.5)\n",
      "Requirement already satisfied: lxml>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from justext>=2.2.0->ReadabilityCalculator) (4.2.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.1->ReadabilityCalculator) (1.15.0)\n",
      "Requirement already satisfied: textstat in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: pyphen in /usr/local/lib/python3.6/dist-packages (from textstat) (0.9.5)\n",
      "Requirement already satisfied: liwc in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: textacy in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
      "Requirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
      "Requirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.1.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.4.1)\n",
      "Requirement already satisfied: spacy<3.0.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.2.4)\n",
      "Requirement already satisfied: scikit-learn<0.24.0,>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.22.2.post1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.5)\n",
      "Requirement already satisfied: jellyfish>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.8.2)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.17.0)\n",
      "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.9.5)\n",
      "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.41.1)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (50.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy) (4.4.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.11.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.3.1)\n",
      "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "!pip install ReadabilityCalculator\n",
    "!pip install textstat\n",
    "!pip install liwc\n",
    "!pip install textacy\n",
    "!pip install vaderSentiment\n",
    "\n",
    "import textacy\n",
    "import textstat\n",
    "from readcalc import readcalc\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "stops = list(set(stopwords.words('english') + list(set(ENGLISH_STOP_WORDS)) + list(set(STOP_WORDS)) + [\"http\"]))\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize\n",
    "import liwc\n",
    "import os\n",
    "\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13385,
     "status": "ok",
     "timestamp": 1604003728779,
     "user": {
      "displayName": "Anu Shrestha",
      "photoUrl": "",
      "userId": "15243898305987665146"
     },
     "user_tz": 360
    },
    "id": "w2D7fA34ANPh",
    "outputId": "63108a63-47e0-4b43-f2be-f436d2c445bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/ECIR 2021 Reproducibility/Code/code_to_publish')\n",
    "\n",
    "import readability\n",
    "import stylistic_features\n",
    "import emotion_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13376,
     "status": "ok",
     "timestamp": 1604003728780,
     "user": {
      "displayName": "Anu Shrestha",
      "photoUrl": "",
      "userId": "15243898305987665146"
     },
     "user_tz": 360
    },
    "id": "JFBdNT2UAWAC"
   },
   "outputs": [],
   "source": [
    "def extract_complexity_emotion_stylistic_features(path, filename, modality_type ):\n",
    "  df = pd.read_csv(os.path.join(path, filename+\".csv\"))\n",
    "  df = df.sample(frac=0.01)\n",
    "  \n",
    "  #Extract complexity features\n",
    "  df = readability.compute_readability(df, 'news_'+modality_type)\n",
    "  df = readability.compute_syntactic(df, 'news_'+modality_type)\n",
    "  df['lexical_diversity'] = df['news_'+modality_type].apply(stylistic_features.lexical_diversity)\n",
    "  df['wlen'] = df['news_'+modality_type].apply(stylistic_features.average_word_length)\n",
    "\n",
    "  # Extract stylistic features\n",
    "  df = stylistic_features.part_of_speech(df, 'news_'+modality_type)\n",
    "  df = stylistic_features.numeric_features(df, 'news_'+modality_type)\n",
    "\n",
    "  # Extract emotion features\n",
    "  emo_dic_path = \"/content/gdrive/My Drive/Emotion_vectors_from_text/emotion_itensity.txt\"\n",
    "  df = emotion_features.emotion_NRC(df, emo_dic_path, 'news_'+modality_type )\n",
    "  df = emotion_features.sentiment_strength_vader(df, 'news_'+modality_type)\n",
    "\n",
    "  # Save extracted features\n",
    "  if not os.path.exists(os.path.join(path,'Generated_features')):\n",
    "    os.makedirs(os.path.join(path,'Generated_features'))\n",
    "\n",
    "  df.to_pickle(os.path.join(path,\"Generated_features/features_for_\"+filename+\"_noLIWC.pkl\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17832,
     "status": "ok",
     "timestamp": 1604003733245,
     "user": {
      "displayName": "Anu Shrestha",
      "photoUrl": "",
      "userId": "15243898305987665146"
     },
     "user_tz": 360
    },
    "id": "cTOYafdxC7OC",
    "outputId": "5a361e1c-3548-4c25-a97a-b1dd9f39dd86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 9\n",
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 27\n",
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 44\n",
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 99\n",
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 79\n",
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 62\n",
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 63\n",
      "TEXT IS NOT BEING PRE PROCESSED\n",
      "type(text) == <class 'str'> ; Size Text: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 35.92it/s]\n",
      "8it [00:00, 1376.20it/s]\n"
     ]
    }
   ],
   "source": [
    "path = '/content/gdrive/My Drive/ECIR 2021 Reproducibility/Data/'\n",
    "filenames = ['title_politifact','text_politifact','title_buzzfeed','text_buzzfeed','title_gossipcop','text_gossipcop']\n",
    "\n",
    "for filename in filenames:\n",
    "  extract_complexity_emotion_stylistic_features(path, filename, modality_type='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 17824,
     "status": "ok",
     "timestamp": 1604003733246,
     "user": {
      "displayName": "Anu Shrestha",
      "photoUrl": "",
      "userId": "15243898305987665146"
     },
     "user_tz": 360
    },
    "id": "qK4qKE_3DJIE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNYo4cRzk3FtlYswhcLWuhi",
   "collapsed_sections": [],
   "name": "step2_extract_complexity_emotion_and_stylistic_features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
