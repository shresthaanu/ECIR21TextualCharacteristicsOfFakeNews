# -*- coding: utf-8 -*-
"""linguistic_features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QwVSZ-Aml0UQFCjr-GADQEx-eLBH2Uno
"""

import nltk
nltk.download("punkt")
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
stops = stopwords.words('english')
nltk.download('averaged_perceptron_tagger')
from nltk.tag import pos_tag
from nltk.tokenize import RegexpTokenizer
from nltk import sent_tokenize
import re
import liwc

from textblob import TextBlob
import pandas as pd
import numpy as np
from collections import Counter
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.metrics import *
import string



def clean_up(s):
  return s.strip(string.punctuation)

def average_word_length(text):
  '''computes average number of words in the text'''
  total_length = sum(len(clean_up(word)) for sentence in str(text) for word in sentence.split())
  num_words = len([clean_up(word) for word in str(text).split()])
  return total_length/num_words

def clean_text(sent):
    lowers = str(sent).lower().split()
    no_punctuation = ["".join(c for c in s if c not in string.punctuation) for s in lowers if s not in stops]
    cleaned_text = " ".join(no_punctuation )
    return cleaned_text

def numeric_features(df, col):
  for ind, row in df.iterrows():
    line = str(row[col])
    df.loc[ind,'percentage_stopwords'] = (len([w for w in line.lower().split(" ") if w in stops])/len([w for w in line.lower().split(" ")]))
    df.loc[ind,'count_uppercased'] = len([w for w in line.split(" ") if w.isupper()])
  return df

def tokenize(text):
    tokenizer = RegexpTokenizer('\w+|\$[\d\.]+|\S+')
    tokens = tokenizer.tokenize(text)
    return tokens

def lexical_diversity(text):
    #Remove URLs
    text = str(text)
    text = re.sub(r"http\S+", "", text)
    text = tokenize(text.lower())
    if len(text) == 0:
        return 0
    return len(set(text)) / len(text)

def part_of_speech(df, col):
  for ind, row in df.iterrows():
    line = str(row[col])
    tagged_sent = pos_tag(line.split())
    df.loc[ind,'num_nouns'] = len([word for word,pos in tagged_sent if pos == 'NN'])
    df.loc[ind,'num_propernouns'] = len([word for word,pos in tagged_sent if pos == 'NNP'])
    df.loc[ind,'num_personalnouns'] = len([word for word,pos in tagged_sent if pos == 'PRP'])
    df.loc[ind,'num_ppssessivenouns'] = len([word for word,pos in tagged_sent if pos == 'PRP$'])
    df.loc[ind,'num_whpronoun'] = len([word for word,pos in tagged_sent if pos == 'WP'])
    df.loc[ind,'num_determinants'] = len([word for word,pos in tagged_sent if pos == 'DT'])
    df.loc[ind,'num_whdeterminants'] = len([word for word,pos in tagged_sent if pos == 'WDT'])
    df.loc[ind,'num_cnum'] = len([word for word,pos in tagged_sent if pos == 'CD'])
    df.loc[ind,'num_adverb'] = len([word for word,pos in tagged_sent if pos == 'RB'])
    df.loc[ind,'num_interjections'] = len([word for word,pos in tagged_sent if pos == 'UH'])
    df.loc[ind,'num_verb'] = len([word for word,pos in tagged_sent if pos == 'VB'])
    df.loc[ind,'num_adj'] = len([word for word,pos in tagged_sent if pos == 'JJ'])
    df.loc[ind,'num_vbd'] = len([word for word,pos in tagged_sent if pos == 'VBD'])
    df.loc[ind,'num_vbg'] = len([word for word,pos in tagged_sent if pos == 'VBG'])
    df.loc[ind,'num_vbn'] = len([word for word,pos in tagged_sent if pos == 'VBN'])
    df.loc[ind,'num_vbp'] = len([word for word,pos in tagged_sent if pos == 'VBP'])
    df.loc[ind,'num_vbz'] = len([word for word,pos in tagged_sent if pos == 'VBZ'])
  return df